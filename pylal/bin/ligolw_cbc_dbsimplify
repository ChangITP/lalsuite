#!/usr/bin/env python

# ============================================================================
#
#                               Preamble
#
# ============================================================================


from optparse import OptionParser
import sqlite3
import sys
import os
import time

from glue.ligolw import lsctables
from glue.ligolw import dbtables
from glue.ligolw.utils import process
from glue import git_version

from pylal import ligolw_sqlutils as sqlutils

__prog__ = "ligolw_cbc_dbsimplify"
__author__ = "Collin Capano <cdcapano@physics.syr.edu>"

description = \
"Cleans and simplifies a database by removing redundant ids."

# ============================================================================
#
#                               Set Options
#
# ============================================================================

def parse_command_line():
    """
    Parser function dedicated
    """
    parser = OptionParser(
        version = git_version.verbose_msg,
        usage   = "%prog [options]",
        description = description
        )
    # following are related to file input and output naming
    parser.add_option( "-d", "--database", action = "store", type = "string", default = None,
        help =
            "Database to update. Must already exist."
            )
    parser.add_option( "-t", "--tmp-space", action = "store", type = "string", default = None,
        metavar = "PATH",
        help =
            "Location of local disk on which to do work. This is optional; " +
            "it is only used to enhance performance in a networked " +
            "environment. "
            )
    parser.add_option( "", "--vacuum", action = "store_true", default = False,
        help = 
            "If turned on, will vacuum the database before saving. " +
            "This cleans any fragmentation and removes empty space " +
            "left behind by all the DELETEs, making the output " +
            "database smaller and more efficient. " +
            "WARNING: Since this requires rebuilding the entire " +
            "database, this can take awhile for larger files." 
            )

    parser.add_option( "-v", "--verbose", action = "store_true", default = False,
        help =
            "Print progress information"
           )
    parser.add_option( "-D", "--debug", action = "store_true", default = False,
        help =
            "Print SQLite queries used and the approximate time taken to run each one." )

    (options, args) = parser.parse_args()
    # check for required options and for self-consistency
    if not options.database:
        raise ValueError, "No database specified."

    return options, sys.argv[1:]


# =============================================================================
#
#                                     Main
#
# =============================================================================

opts, args = parse_command_line()

# get input database filename
filename = opts.database
if not os.path.isfile( filename ):
    raise ValueError, "The input file, %s, cannot be found." % filename

# Setup working databases and connections
if opts.verbose: 
    print >> sys.stdout, "Creating a database connection..."
working_filename = dbtables.get_connection_filename( 
    filename, tmp_path = opts.tmp_space, verbose = opts.verbose )
connection = sqlite3.connect( working_filename )
if opts.tmp_space:
    dbtables.set_temp_store_directory(connection, opts.tmp_space, verbose = opts.verbose)
dbtables.DBTable_set_connection( connection )

# Add program to process and process params table

# FIXME: remove the following two lines once boolean type
# has been properly handled
from glue.ligolw import types as ligolwtypes
ligolwtypes.FromPyType[type(True)] = ligolwtypes.FromPyType[type(8)]

xmldoc = dbtables.get_xml(connection)
proc_id = process.register_to_xmldoc(xmldoc, 'ligolw_cbc_dbsimplify', opts.__dict__, version = git_version.id)

# create needed functions
connection.create_function("concat_2cols", 2, sqlutils.concatenate)
connection.create_function("concat_5cols", 5, sqlutils.concatenate)
connection.create_function("concat_7cols", 7, sqlutils.concatenate)

# create needed indices on tables if they don't already exist
current_indices = [index[0] for index in 
    connection.cursor().execute('SELECT name FROM sqlite_master WHERE type == "index"').fetchall()]
sqlscript = ''
if 'ts_io_index' not in current_indices:
    sqlscript += 'CREATE INDEX ts_io_index ON time_slide (instrument, offset);\n'
if 'e_sgitlc_index' not in current_indices:
    sqlscript += 'CREATE INDEX e_sgitlc_index ON experiment (search, search_group, instruments, gps_start_time, gps_end_time, lars_id, comments);\n'
if 'es_etvds_index' not in current_indices:
    sqlscript += 'CREATE INDEX es_etvds_index ON experiment_summary (experiment_id, time_slide_id, veto_def_name, datatype, sim_proc_id);\n'
if 'em_esi_index' not in current_indices:
    sqlscript += 'CREATE INDEX em_esi_index ON experiment_map (experiment_summ_id);\n'
if 'em_cei_index' not in current_indices:
    sqlscript += 'CREATE INDEX em_cei_index ON experiment_map (coinc_event_id);\n'

if sqlscript != '':
    if opts.verbose:
        print >> sys.stdout, "Creating needed indices..."
    connection.cursor().executescript(sqlscript)


# =============================== SEGMENTS TABLES ===================== #

if opts.verbose:
   print >> sys.stdout, "Cleaning up the segments tables..."

sqlscript = """
--
-- Cleaning up the segments tables as well as the associated
-- entries in the process & process_params tables
-- 

-- Create a table that combines the info from the segment_definer &
-- segment_summary tables. Then for each unique set of (ifo, cat_vers, times)
-- keep only the lowest process_id.
CREATE TEMP TABLE segments_tbl AS
    SELECT
        segment_definer.process_id AS proc_id,
        segment_definer.segment_def_id AS segdef_id,
        segment_definer.ifos AS ifo,
        concat_2cols(segment_definer.name, segment_definer.version) AS cat_vers,
        concat_2cols(segment_summary.start_time, segment_summary.end_time) AS times
    FROM
        segment_definer
        JOIN segment_summary ON (
            segment_definer.process_id == segment_summary.process_id
            AND segment_definer.segment_def_id == segment_summary.segment_def_id)
    GROUP BY proc_id;

DELETE FROM segments_tbl
    WHERE proc_id NOT IN (
        SELECT MIN(proc_id)
        FROM segments_tbl
        GROUP BY ifo, cat_vers, times);

-- Remove duplicate entries related to vetoes and segments in the process 
-- & process_params tables
DELETE FROM process
    WHERE
        process_id NOT IN (SELECT proc_id FROM segments_tbl)
        AND program == ".executables/ligolw_segments_from_cats";
DELETE FROM process_params
    WHERE
        process_id NOT IN (SELECT proc_id FROM segments_tbl)
        AND program == ".executables/ligolw_segments_from_cats";

-- Remove duplicate entries in the segment, segment_summary & 
-- segment_definer tables
DELETE FROM segment 
    WHERE process_id NOT IN (SELECT proc_id FROM segments_tbl);
DELETE FROM segment_definer 
    WHERE process_id NOT IN (SELECT proc_id FROM segments_tbl);
DELETE FROM segment_summary
    WHERE process_id NOT IN (SELECT proc_id FROM segments_tbl);

DROP TABLE segments_tbl;
"""
if opts.debug:
    print >> sys.stderr, sqlscript
    print >> sys.stderr, time.localtime()[3], time.localtime()[4], time.localtime()[5]
connection.cursor().executescript( sqlscript )
if opts.debug:
    print >> sys.stderr, time.localtime()[3], time.localtime()[4], time.localtime()[5]


# ==================================== VETO DEFINER TABLES ====================== #
# == NB - THESE ARE NOT USED FOR ANYTHING IN PIPEDOWN SO GET RID OF EM ========== #

if opts.verbose:
    print >> sys.stdout, "Removing the veto_definer table..."

sqlscript = 'DROP TABLE veto_definer;'

if opts.debug:
    print >> sys.stderr, sqlscript
    print >> sys.stderr, time.localtime()[3], time.localtime()[4], time.localtime()[5]

try: connection.cursor().executescript( sqlscript )
except: print "Warning (not serious): there was no veto definer table"

if opts.debug:
    print >> sys.stderr, time.localtime()[3], time.localtime()[4], time.localtime()[5]


# ================================= PROCESS / PROCESS PARAMS TABLES ================ #

if opts.verbose:
   print >> sys.stdout, "Collect useful information from process and process_params tables"

sqlscript = """
CREATE TEMP TABLE proc_params AS
    SELECT
        process.process_id AS proc_id,
        process.program AS program,
        group_concat(pp_table.value) AS value,
        group_concat(pp_table.param) AS params,
        concat_5cols(process.start_time, process.end_time, process.username,
            process.node, process.version) AS process_info
    FROM
        process_params AS pp_table
        JOIN process ON (
            pp_table.process_id == process.process_id)
    GROUP BY proc_id;

CREATE TEMP TABLE _pidmap_ AS
    SELECT
        old_pp_table.proc_id AS old_pid,
        MIN(new_pp_table.proc_id) AS new_pid,
        old_pp_table.program AS program
    FROM
        proc_params AS old_pp_table
        JOIN proc_params AS new_pp_table ON (
            old_pp_table.value == new_pp_table.value
            AND old_pp_table.process_info == new_pp_table.process_info
            AND old_pp_table.params == new_pp_table.params
            AND old_pp_table.program == new_pp_table.program)
    GROUP BY old_pid;

CREATE INDEX pidmap_index ON _pidmap_ (old_pid);

"""
if opts.debug:
    print >> sys.stderr, sqlscript
    print >> sys.stderr, time.localtime()[3], time.localtime()[4], time.localtime()[5]
connection.cursor().executescript( sqlscript )
if opts.debug:
    print >> sys.stderr, time.localtime()[3], time.localtime()[4], time.localtime()[5]


# =================================== TIME SLIDE TABLE ======================= #

if opts.verbose:
    print >> sys.stdout, "Cleaning up the time_slide table ..."

sqlscript = """
CREATE TEMP TABLE all_slides AS
SELECT
    time_slide_id AS id,
	concat_2cols(instrument, offset) AS slide 
FROM
    time_slide
GROUP BY
    time_slide_id;

CREATE TEMP TABLE distinct_slides AS
    SELECT
        MIN(id) AS id,
        slide
    FROM
        all_slides
    GROUP BY
        slide;

CREATE TEMP TABLE _idmap_ AS
    SELECT 
        distinct_slides.id AS new,
        all_slides.id AS old
    FROM
        all_slides
    JOIN
        distinct_slides ON (
            distinct_slides.slide == all_slides.slide
        );

CREATE INDEX idm_o_index ON _idmap_ (old);
UPDATE coinc_event SET time_slide_id = (SELECT new FROM _idmap_ WHERE old == time_slide_id);
UPDATE experiment_summary SET time_slide_id = (SELECT new FROM _idmap_ WHERE old == time_slide_id);

DELETE FROM time_slide WHERE time_slide_id IN (SELECT old FROM _idmap_ WHERE old != new);

DROP TABLE all_slides;
DROP TABLE _idmap_;
"""
if opts.debug:
    print >> sys.stderr, sqlscript
    print >> sys.stderr, time.localtime()[3], time.localtime()[4], time.localtime()[5]
connection.cursor().executescript( sqlscript )
if opts.debug:
    print >> sys.stderr, time.localtime()[3], time.localtime()[4], time.localtime()[5]


# ==================================== COINC DEFINER TABLE ================= #

if opts.verbose:
    print >> sys.stdout, "Cleaning up the coinc_definer table..."

sqlscript = """
--
-- Cleaning up the coinc_definer table
--

-- Create a table that maps the coinc_definer_ids of redundant entries
-- to those entries one is going to keep.
CREATE TEMP TABLE _cdidmap_ AS
    SELECT
        old_cd_table.coinc_def_id AS old_cdid,
        MIN(new_cd_table.coinc_def_id) AS new_cdid
    FROM
        coinc_definer AS old_cd_table
        JOIN coinc_definer AS new_cd_table ON (
            new_cd_table.search == old_cd_table.search
            AND new_cd_table.search_coinc_type == old_cd_table.search_coinc_type
        )
    GROUP BY old_cdid;

CREATE INDEX cdidmap_index ON _cdidmap_ (old_cdid);

-- Update the coinc_event table with new coinc_def_ids
UPDATE coinc_event 
    SET coinc_def_id = (
        SELECT new_cdid 
        FROM _cdidmap_ 
        WHERE old_cdid == coinc_def_id);

DROP INDEX cdidmap_index;

-- Remove redundant entries in the coinc_definer table
DELETE FROM coinc_definer
     WHERE coinc_def_id IN (
     SELECT old_cdid 
     FROM _cdidmap_
     WHERE old_cdid != new_cdid);

DROP TABLE _cdidmap_;
"""
if opts.debug:
    print >> sys.stderr, sqlscript
    print >> sys.stderr, time.localtime()[3], time.localtime()[4], time.localtime()[5]
connection.cursor().executescript( sqlscript )
if opts.debug:
    print >> sys.stderr, time.localtime()[3], time.localtime()[4], time.localtime()[5]


# =================================== SIMULATION TABLES ===================== #

if opts.verbose:
    print >> sys.stdout, "Checking whether this database has simulation tables..."

programs = connection.cursor().execute('SELECT program FROM process GROUP BY program').fetchall()
for idx, entry in enumerate(programs):
    programs[idx] = entry[0]

if len(set(['inspinj','rinj']) & set(programs)) != 0:
    if opts.verbose:
        print >> sys.stdout, "Cleaning simulation tables..."
    
    sqlscript = """
    --
    -- Simulation Tables Clean-up
    --

    -- Update sim_proc_ids in the experiment_summary table
    UPDATE experiment_summary 
        SET sim_proc_id = (
            SELECT new_pid 
            FROM _pidmap_ 
            WHERE old_pid == sim_proc_id);

    """
    if 'inspinj' in programs:
        sqlscript += """
        -- Delete duplicate entries in the simulation tables
        DELETE FROM sim_inspiral 
            WHERE process_id IN (
                SELECT old_pid 
                FROM _pidmap_ 
                WHERE 
                    old_pid != new_pid
                    AND program == "inspinj");
        """
    if 'rinj' in programs:
        sqlscript += """
        DELETE FROM sim_ringdown 
            WHERE process_id IN (
                SELECT old_pid 
                FROM _pidmap_ 
                WHERE 
                    old_pid != new_pid
                    AND program == "rinj");
        """
    if opts.debug:
        print >> sys.stderr, sqlscript
        print >> sys.stderr, time.localtime()[3], time.localtime()[4], time.localtime()[5]
    connection.cursor().executescript( sqlscript )
    if opts.debug:
        print >> sys.stderr, time.localtime()[3], time.localtime()[4], time.localtime()[5]


# =================== EXPERIMENT / EXPERIMENT SUMMARY TABLES ========================= #
# ============== NB DIFFERENT FROM SNGLSTAGE BRANCH ================================== #
# ============== INSTEAD REVERT TO HOW THE FUNCTION LOOKS ON MASTER ================== #
# ============== REMOVE LARS_ID CONDITION AS LARS IS NOT USED ======================== #

if opts.verbose:
    print >> sys.stdout, "Cleaning experiment tables..."

sqlscript = """
--
-- Experiment Tables clean up
--

-- experiment table clean up:
-- create map table to map experiment_ids that are to be kept
-- to experiment ids that are to be discarded, in the same manner
-- as done above
CREATE TEMP TABLE eidmap AS
    SELECT
        old_exp.experiment_id AS old_eid,
        MIN(new_exp.experiment_id) AS new_eid
    FROM
        experiment AS old_exp
        JOIN experiment AS new_exp ON (
            (new_exp.search == old_exp.search
              OR new_exp.search IS NULL AND old_exp.search IS NULL)
            AND new_exp.search_group == old_exp.search_group
            AND new_exp.instruments == old_exp.instruments
            AND new_exp.gps_start_time == old_exp.gps_start_time
            AND new_exp.gps_end_time == old_exp.gps_end_time
                AND ( new_exp.comments == old_exp.comments
                OR (new_exp.comments IS NULL AND old_exp.comments IS NULL) )
        )
    GROUP BY
        old_exp.experiment_id;
DROP INDEX e_sgitlc_index;
-- delete the old ids from the experiment table
DELETE FROM experiment WHERE experiment_id IN (SELECT old_eid FROM eidmap WHERE old_eid != new_eid);

-- update the experiment_ids in the experiment summary table
CREATE INDEX em_old_index ON eidmap (old_eid);
UPDATE experiment_summary
    SET experiment_id = (
        SELECT new_eid 
        FROM eidmap
        WHERE experiment_summary.experiment_id == old_eid );

-- experiment summary clean up

-- create a table to store the distinct experiment_summaries
CREATE TEMP TABLE distinct_experiments (experiment_summ_id, experiment_id, time_slide_id, veto_def_name, datatype, sim_proc_id);
INSERT INTO distinct_experiments (experiment_id, time_slide_id, veto_def_name, datatype, sim_proc_id) 
    SELECT DISTINCT
            experiment_summary.experiment_id,
            experiment_summary.time_slide_id,
            experiment_summary.veto_def_name,
            experiment_summary.datatype,
            experiment_summary.sim_proc_id
    FROM    
        experiment_summary;
CREATE INDEX de_etvds_index ON distinct_experiments (experiment_id, time_slide_id, veto_def_name, datatype, sim_proc_id);

-- get one unique experiment_summ_id for each row in distinct_experiments
UPDATE
    distinct_experiments
SET
    experiment_summ_id = (
        SELECT
            experiment_summ_id
        FROM
            experiment_summary
        WHERE 
            experiment_summary.experiment_id == distinct_experiments.experiment_id
            AND experiment_summary.time_slide_id == distinct_experiments.time_slide_id
            AND (
	      experiment_summary.veto_def_name == distinct_experiments.veto_def_name
		OR (
	          experiment_summary.veto_def_name IS NULL AND distinct_experiments.veto_def_name IS NULL))	
            AND experiment_summary.datatype == distinct_experiments.datatype
            AND (
              experiment_summary.sim_proc_id == distinct_experiments.sim_proc_id
              OR (
                experiment_summary.sim_proc_id IS NULL AND distinct_experiments.sim_proc_id IS NULL )));

-- create an table to map esids to be deleted to esids to be saved
CREATE TEMP TABLE esidmap AS
    SELECT
        experiment_summary.experiment_summ_id AS old_esid,
        distinct_experiments.experiment_summ_id AS new_esid
    FROM
        experiment_summary
    JOIN distinct_experiments ON (
        experiment_summary.experiment_id == distinct_experiments.experiment_id
        AND experiment_summary.time_slide_id == distinct_experiments.time_slide_id
        AND (
          experiment_summary.veto_def_name == distinct_experiments.veto_def_name
	OR (
	  experiment_summary.veto_def_name IS NULL AND distinct_experiments.veto_def_name IS NULL))	
        AND experiment_summary.datatype == distinct_experiments.datatype
        AND (
          experiment_summary.sim_proc_id == distinct_experiments.sim_proc_id
          OR (
            experiment_summary.sim_proc_id IS NULL AND distinct_experiments.sim_proc_id IS NULL )));
DROP INDEX es_etvds_index;
CREATE INDEX esm_new_index ON esidmap (new_esid);
CREATE INDEX esm_old_index ON esidmap (old_esid);

-- sum durations and nevents
CREATE TEMP TABLE sum_dur_nevents AS
    SELECT esidmap.new_esid AS esid, 
    SUM(experiment_summary.duration) AS sum_dur, 
    SUM(experiment_summary.nevents) AS sum_nevents
    FROM esidmap
    JOIN experiment_summary ON (esidmap.old_esid == experiment_summary.experiment_summ_id)
    GROUP BY esid;
CREATE INDEX sdn_esid_index ON sum_dur_nevents (esid);

-- delete the old ids from the experiment_summary table
DELETE FROM experiment_summary WHERE experiment_summ_id IN (SELECT old_esid FROM esidmap WHERE old_esid != new_esid);

-- update the durations and the nevents
UPDATE experiment_summary
SET duration = (
        SELECT sum_dur
        FROM sum_dur_nevents
        WHERE sum_dur_nevents.esid == experiment_summary.experiment_summ_id ),
    nevents = (
        SELECT sum_nevents
        FROM sum_dur_nevents
        WHERE sum_dur_nevents.esid == experiment_summary.experiment_summ_id );

-- update the experiment_map table
UPDATE experiment_map
SET experiment_summ_id = (
    SELECT new_esid FROM esidmap
    WHERE experiment_map.experiment_summ_id == old_esid );
"""
if opts.debug:
    print >> sys.stderr, sqlscript
    print >> sys.stderr, time.localtime()[3], time.localtime()[4], time.localtime()[5]
connection.cursor().executescript( sqlscript )
if opts.debug:
    print >> sys.stderr, time.localtime()[3], time.localtime()[4], time.localtime()[5]


# ======================== DELETE STUFF FROM PROCESS / PROCESS PARAMS TABLES ======= #

if opts.verbose:
    print >> sys.stdout, "Deleting redundant rows in the process & process_params tables"

sqlscript = """
-- Remove redundant process rows
DELETE FROM process 
    WHERE process_id IN (
        SELECT old_pid 
        FROM _pidmap_ 
        WHERE old_pid != new_pid
            AND (program = "inspinj"
                OR program = "rinj"
                OR program = "ligolw_tisi") );
DELETE FROM process_params 
        WHERE process_id IN (
        SELECT old_pid 
        FROM _pidmap_ 
        WHERE old_pid != new_pid
            AND (program = "inspinj"
                OR program = "rinj"
                OR program = "ligolw_tisi") );

DROP INDEX pidmap_index;
DROP TABLE _pidmap_;
"""
if opts.debug:
    print >> sys.stderr, sqlscript
    print >> sys.stderr, time.localtime()[3], time.localtime()[4], time.localtime()[5]
connection.cursor().executescript( sqlscript )
if opts.debug:
    print >> sys.stderr, time.localtime()[3], time.localtime()[4], time.localtime()[5]

# Vacuum database if desired
if opts.vacuum:
    if opts.verbose:
        print >> sys.stderr, "Vacuuming database..."
    connection.cursor().execute( 'VACUUM' )

#
#       Save and Exit
#

connection.commit()
connection.close()

# write output database
dbtables.put_connection_filename(filename, working_filename, verbose = opts.verbose)

if opts.verbose: 
    print >> sys.stdout, "Finished!"

# set process end time
process.set_process_end_time(proc_id)
sys.exit(0)


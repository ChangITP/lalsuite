#!/usr/bin/env python

# ============================================================================
#
#                               Preamble
#
# ============================================================================


from optparse import OptionParser
import sqlite3
import sys
import os
import time

from glue.ligolw import lsctables
from glue.ligolw import dbtables
from glue.ligolw.utils import process
from glue import git_version

from pylal import ligolw_sqlutils as sqlutils

__prog__ = "ligolw_cbc_dbsimplify"
__author__ = "Collin Capano <cdcapano@physics.syr.edu>"

description = \
"Cleans and simplifies a database by removing redundant ids."

# ============================================================================
#
#                               Set Options
#
# ============================================================================

def parse_command_line():
    """
    Parser function dedicated
    """
    parser = OptionParser(
        version = git_version.verbose_msg,
        usage   = "%prog [options]",
        description = description
        )
    # following are related to file input and output naming
    parser.add_option( "-d", "--database", action = "store", type = "string", default = None,
        help =
            "Database to update. Must already exist."
            )
    parser.add_option( "-t", "--tmp-space", action = "store", type = "string", default = None,
        metavar = "PATH",
        help =
            "Location of local disk on which to do work. This is optional; " +
            "it is only used to enhance performance in a networked " +
            "environment. "
            )
    parser.add_option( "", "--vacuum", action = "store_true", default = False,
        help = 
            "If turned on, will vacuum the database before saving. " +
            "This cleans any fragmentation and removes empty space " +
            "left behind by all the DELETEs, making the output " +
            "database smaller and more efficient. " +
            "WARNING: Since this requires rebuilding the entire " +
            "database, this can take awhile for larger files." 
            )

    parser.add_option( "-v", "--verbose", action = "store_true", default = False,
        help =
            "Print progress information"
           )
    parser.add_option( "-D", "--debug", action = "store_true", default = False,
        help =
            "Print SQLite queries used and the approximate time taken to run each one." )

    (options, args) = parser.parse_args()
    # check for required options and for self-consistency
    if not options.database:
        raise ValueError, "No database specified."

    return options, sys.argv[1:]


# =============================================================================
#
#                                     Main
#
# =============================================================================

opts, args = parse_command_line()

# get input database filename
filename = opts.database
if not os.path.isfile( filename ):
    raise ValueError, "The input file, %s, cannot be found." % filename

# Setup working databases and connections
if opts.verbose: 
    print >> sys.stdout, "Creating a database connection..."
working_filename = dbtables.get_connection_filename( 
    filename, tmp_path = opts.tmp_space, verbose = opts.verbose )
connection = sqlite3.connect( working_filename )
if opts.tmp_space:
    dbtables.set_temp_store_directory(connection, opts.tmp_space, verbose = opts.verbose)
dbtables.DBTable_set_connection( connection )

# Add program to process and process params table

# FIXME: remove the following two lines once boolean type
# has been properly handled
from glue.ligolw import types as ligolwtypes
ligolwtypes.FromPyType[type(True)] = ligolwtypes.FromPyType[type(8)]

xmldoc = dbtables.get_xml(connection)
proc_id = process.register_to_xmldoc(xmldoc, 'ligolw_cbc_dbsimplify', opts.__dict__, version = git_version.id)

# create needed functions
connection.create_aggregate("concatenate", 2, sqlutils.aggregate_concatenate)

# create needed indices on tables if they don't already exist
current_indices = [index[0] for index in 
    connection.cursor().execute('SELECT name FROM sqlite_master WHERE type == "index"').fetchall()]
sqlscript = ''
if 'ts_io_index' not in current_indices:
    sqlscript += 'CREATE INDEX ts_io_index ON time_slide (instrument, offset);\n'
if 'e_sgitlc_index' not in current_indices:
    sqlscript += 'CREATE INDEX e_sgitlc_index ON experiment (search, search_group, instruments, gps_start_time, gps_end_time, lars_id, comments);\n'
if 'es_etvds_index' not in current_indices:
    sqlscript += 'CREATE INDEX es_etvds_index ON experiment_summary (experiment_id, time_slide_id, veto_def_name, datatype, sim_proc_id);\n'
if 'em_esi_index' not in current_indices:
    sqlscript += 'CREATE INDEX em_esi_index ON experiment_map (experiment_summ_id);\n'
if 'em_cei_index' not in current_indices:
    sqlscript += 'CREATE INDEX em_cei_index ON experiment_map (coinc_event_id);\n'

if sqlscript != '':
    if opts.verbose:
        print >> sys.stdout, "Creating needed indices..."
    connection.cursor().executescript(sqlscript)

if verbose
   print >> sys.stdout, "Cleaning up the veto_definer and segments tables..."

sqlscript = """
--
-- Cleaning up the veto_definer and segments tables as well as the associated
-- entries in the process & process_params tables
-- 

-- Create a table that combines the info from the segment_definer &
-- segment_summary tables. Then for each unique set of (ifo, cat_vers, times)
-- keep only the lowest process_id.
CREATE TEMP TABLE segments_tbl AS
    SELECT
        segment_definer.process_id AS proc_id,
        segment_definer.segment_def_id AS segdef_id,
        segment_definer.ifos AS ifo,
        concatenate(segment_definer.name, segment_definer.version) AS cat_vers,
        concatenate(segment_summary.start_time, segment_summary.end_time) AS times
    FROM
        segment_definer
        JOIN segment_summary ON (
            segment_definer.process_id == segment_summary.process_id
            AND segment_definer.segment_def_id == segment_summary.segment_def_id)
    GROUP BY proc_id;

DELETE FROM segments_tbl
    WHERE proc_id NOT IN (
        SELECT MIN(proc_id)
        FROM segments_tbl
        GROUP BY ifo, cat_vers, times);

-- Remove duplicate entries related to vetoes and segments in the process 
-- & process_params tables
DELETE FROM process
    WHERE
        process_id NOT IN (SELECT proc_id FROM segments_tbl)
        AND program == ".executables/ligolw_segments_from_cats";
DELETE FROM process_params
    WHERE
        process_id NOT IN (SELECT proc_id FROM segments_tbl)
        AND program == ".executables/ligolw_segments_from_cats";

-- Remove duplicate entries in the segment, segment_summary & 
-- segment_definer tables
DELETE FROM segment 
    WHERE process_id NOT IN (SELECT proc_id FROM segments_tbl);
DELETE FROM segment_definer 
    WHERE process_id NOT IN (SELECT proc_id FROM segments_tbl);
DELETE FROM segment_summary
    WHERE process_id NOT IN (SELECT proc_id FROM segments_tbl);

-- Remove duplicate entries in the veto_definer table
DELETE FROM veto_definer
    WHERE process_id NOT IN (SELECT proc_id FROM segments_tbl);

DROP TABLE segments_tbl;
"""

if opts.debug:
    print >> sys.stderr, sqlscript
    print >> sys.stderr, time.localtime()[3], time.localtime()[4], time.localtime()[5]

connection.cursor().executescript( sqlscript )

if opts.debug:
    print >> sys.stderr, time.localtime()[3], time.localtime()[4], time.localtime()[5]

if opts.verbose:
    print >> sys.stdout, "Clean up the time_slide table ..."

sqlscript = """
--
-- Cleaning up the time_slide table and its related entries in the process
-- & process_params tables
--

-- Create a table that maps the process_ids of redundant time_slide entries
-- to those entries one is going to keep.
CREATE TEMP TABLE _pidmap_ AS
    SELECT
        old_pp_table.process_id AS old_pid, 
        MIN(new_pp_table.process_id) AS new_pid
    FROM
        process_params AS old_pp_table
        JOIN process_params AS new_pp_table ON (
            new_pp_table.value == old_pp_table.value)       
    WHERE
        old_pp_table.program == "ligolw_tisi"
        AND old_pp_table.param == "--inspiral-num-slides"
    GROUP BY old_pid;

CREATE INDEX pidmap_index ON _pidmap_ (old_pid);

-- Update the process_ids in the time_slide table with the new ids from _pidmap_
UPDATE time_slide 
    SET process_id = (
        SELECT new_pid 
        FROM _pidmap_ 
        WHERE old_pid == process_id);

DROP INDEX pidmap_index;

-- Delete redundant entries in the process and process_params tables
DELETE FROM process 
    WHERE process_id IN (
        SELECT old_pid 
        FROM _pidmap_ 
        WHERE old_pid != new_pid);
DELETE FROM process_params 
    WHERE process_id IN (
        SELECT old_pid 
        FROM _pidmap_ 
        WHERE old_pid != new_pid);

DROP TABLE _pidmap_;

-- Create a table that combines the information about a single time_slide into 
-- a single row.  This makes comparison between time_slides easier.
CREATE TEMP TABLE compact_time_slide AS
    SELECT
        time_slide_id AS tsid,
        process_id AS pid,
        group_concat(instrument) AS ifos,
        group_concat(offset) AS offset
    FROM time_slide
    GROUP BY time_slide_id;

-- Create a table that maps the time_slide_ids of redundant time_slide entries
-- to those entries one is going to keep.
CREATE TEMP TABLE _tsidmap_ AS
    SELECT
        old_ts_table.tsid AS old_tsid,
        MIN(new_ts_table.tsid) AS new_tsid
    FROM
        compact_time_slide AS old_ts_table
        JOIN compact_time_slide AS new_ts_table ON (
            new_ts_table.pid == old_ts_table.pid
            AND new_ts_table.ifos == old_ts_table.ifos
            AND new_ts_table.offset == old_ts_table.offset)
    GROUP BY old_tsid;

DROP TABLE compact_time_slide;

CREATE INDEX tsidmap_index ON _tsidmap_ (old_tsid);

-- Update the coinc_event and experiment_summary tables with new time_slide_ids
UPDATE coinc_event 
    SET time_slide_id = (
        SELECT new_tsid 
        FROM _tsidmap_ 
        WHERE old_tsid == time_slide_id);
UPDATE experiment_summary
    SET time_slide_id = (
        SELECT new_tsid 
        FROM _tsidmap_ 
        WHERE old_tsid == time_slide_id);

DROP INDEX tsidmap_index;

-- Delete the redundant entries in the time_slide table
DELETE FROM time_slide 
    WHERE time_slide_id IN (
        SELECT old_tsid 
        FROM _tsidmap_ 
        WHERE old_tsid != new_tsid);

DROP TABLE _tsidmap_;

"""

if opts.debug:
    print >> sys.stderr, sqlscript
    print >> sys.stderr, time.localtime()[3], time.localtime()[4], time.localtime()[5]

connection.cursor().executescript( sqlscript )

if opts.debug:
    print >> sys.stderr, time.localtime()[3], time.localtime()[4], time.localtime()[5]

if opts.verbose:
    print >> sys.stdout, "Cleaning up the coinc_definer table..."

sqlscript = """
--
-- Cleaning up the coinc_definer table
--

-- Create a table that maps the coinc_definer_ids of redundant entries
-- to those entries one is going to keep.
CREATE TEMP TABLE _cdidmap_ AS
    SELECT
        old_cd_table.coinc_def_id AS old_cdid,
        MIN(new_cd_table.coinc_def_id) AS new_cdid
    FROM
        coinc_definer AS old_cd_table
        JOIN coinc_definer AS new_cd_table ON (
            new_cd_table.search == old_cd_table.search
            AND new_cd_table.search_coinc_type == old_cd_table.search_coinc_type
        )
    GROUP BY old_cdid;

CREATE INDEX cdidmap_index ON _cdidmap_ (old_cdid);

-- Update the coinc_event table with new coinc_def_ids
UPDATE coinc_event 
    SET coinc_def_id = (
        SELECT new_cdid 
        FROM _cdidmap_ 
        WHERE old_cdid == coinc_def_id);

DROP INDEX cdidmap_index;

-- Remove redundant entries in the coinc_definer table
DELETE FROM coinc_definer
     WHERE coinc_def_id IN (
     SELECT old_cdid 
     FROM _cdidmap_
     WHERE old_cdid != new_cdid);

DROP TABLE _cdidmap_;
"""

if opts.debug:
    print >> sys.stderr, sqlscript
    print >> sys.stderr, time.localtime()[3], time.localtime()[4], time.localtime()[5]

connection.cursor().executescript( sqlscript )

if opts.debug:
    print >> sys.stderr, time.localtime()[3], time.localtime()[4], time.localtime()[5]


if opts.verbose:
    print >> sys.stdout, "Cleaning experiment tables..."

sqlscript = """
--
-- Experiment Tables clean up
--

-- experiment table clean up:
-- create map table to map experiment_ids that are to be kept
-- to experiment ids that are to be discarded, in the same manner
-- as done above

CREATE TEMP TABLE _eidmap_ AS
    SELECT
        old_exp.experiment_id AS old_eid,
        MIN(new_exp.experiment_id) AS new_eid
    FROM
        experiment AS old_exp
        JOIN experiment AS new_exp ON (
            (new_exp.search == old_exp.search
                OR new_exp.search IS NULL AND old_exp.search IS NULL)
            AND new_exp.search_group == old_exp.search_group
            AND new_exp.instruments == old_exp.instruments
            AND new_exp.gps_start_time == old_exp.gps_start_time
            AND new_exp.gps_end_time == old_exp.gps_end_time
            AND (new_exp.lars_id == old_exp.lars_id
                OR (new_exp.lars_id IS NULL AND old_exp.lars_id IS NULL))
            AND (new_exp.comments == old_exp.comments
                OR (new_exp.comments IS NULL AND old_exp.comments IS NULL)) )
    GROUP BY old_exp.experiment_id;

DROP INDEX e_sgitlc_index;

-- delete the old ids from the experiment table
DELETE FROM experiment 
    WHERE experiment_id IN (
        SELECT old_eid 
        FROM _eidmap_
        WHERE old_eid != new_eid);

-- update the experiment_ids in the experiment summary table
CREATE INDEX em_old_index ON _eidmap_ (old_eid);
UPDATE experiment_summary
    SET experiment_id = (
        SELECT new_eid 
        FROM _eidmap_
        WHERE experiment_summary.experiment_id == old_eid);

DROP INDEX em_old_index;

-- experiment summary clean up

-- create an table to map esids to be deleted to esids to be saved
CREATE TEMP TABLE _esidmap_ AS
    SELECT
        old_expsumm.experiment_summ_id AS old_esid,
        MIN(new_expsumm.experiment_summ_id) AS new_esid
    FROM 
        experiment_summary AS old_expsumm
        JOIN experiment_summary AS new_expsumm ON (
            old_expsumm.experiment_id == new_expsumm.experiment_id
            AND old_expsumm.time_slide_id == new_expsumm.time_slide_id
            AND old_expsumm.veto_def_name == new_expsumm.veto_def_name
            AND old_expsumm.datatype == new_expsumm.datatype
            AND (old_expsumm.sim_proc_id == new_expsumm.sim_proc_id
                OR (old_expsumm.sim_proc_id IS NULL AND new_expsumm.sim_proc_id IS NULL)) ) 
    GROUP BY old_esid;

DROP INDEX es_etvds_index;
CREATE INDEX esidmap_index on _esidmap_ (old_esid, new_esid);

-- sum durations and nevents
CREATE TEMP TABLE sum_dur_nevents AS
    SELECT
        _esidmap_.new_esid AS esid, 
        SUM(experiment_summary.duration) AS sum_dur, 
        SUM(experiment_summary.nevents) AS sum_nevents
    FROM _esidmap_
        JOIN experiment_summary ON (
            _esidmap_.old_esid == experiment_summary.experiment_summ_id)
    GROUP BY esid;

CREATE INDEX sdn_esid_index ON sum_dur_nevents (esid);

-- delete the old ids from the experiment_summary table
DELETE FROM experiment_summary
    WHERE experiment_summ_id IN (
        SELECT old_esid
        FROM _esidmap_
        WHERE old_esid != new_esid);

-- update the durations and the nevents
UPDATE experiment_summary
    SET duration = (
        SELECT sum_dur
        FROM sum_dur_nevents
        WHERE sum_dur_nevents.esid == experiment_summary.experiment_summ_id),
    nevents = (
        SELECT sum_nevents
        FROM sum_dur_nevents
        WHERE sum_dur_nevents.esid == experiment_summary.experiment_summ_id);

DROP INDEX sdn_esid_index;

-- update the experiment_map table
UPDATE experiment_map
    SET experiment_summ_id = (
        SELECT new_esid 
        FROM _esidmap_
        WHERE experiment_map.experiment_summ_id == old_esid);

DROP INDEX esidmap_index
DROP TABLE _esidmap_
"""
if opts.debug:
    print >> sys.stderr, sqlscript
    print >> sys.stderr, time.localtime()[3], time.localtime()[4], time.localtime()[5]

connection.cursor().executescript( sqlscript )

if opts.debug:
    print >> sys.stderr, time.localtime()[3], time.localtime()[4], time.localtime()[5]

# Vacuum database if desired
if opts.vacuum:
    if opts.verbose:
        print >> sys.stderr, "Vacuuming database..."
    connection.cursor().execute( 'VACUUM' )

#
#       Save and Exit
#

connection.commit()
connection.close()

# write output database
dbtables.put_connection_filename(filename, working_filename, verbose = opts.verbose)

if opts.verbose: 
    print >> sys.stdout, "Finished!"

# set process end time
process.set_process_end_time(proc_id)
sys.exit(0)

